#!/bin/bash

# assemble.sh generated by masurca
CONFIG_PATH="/home/shared/programs/MaSuRCA-3.3.0/C2/geosiphon_config"
CMD_PATH="/home/shared/programs/MaSuRCA-3.3.0/bin/masurca"
set -o pipefail

# Test that we support <() redirection
(eval "cat <(echo test) >/dev/null" 2>/dev/null) || {
  echo >&2 "ERROR: The shell used is missing important features."
  echo >&2 "       Run the assembly script directly as './$0'"
  exit 1
}

# Parse command line switches
while getopts ":rc" o; do
  case "${o}" in
    c)
    echo "configuration file is '$CONFIG_PATH'"
    exit 0
    ;;
    r)
    echo "Rerunning configuration"
    exec perl "$CMD_PATH" "$CONFIG_PATH"
    echo "Failed to rerun configuration"
    exit 1
    ;;
    *)
    echo "Usage: $0 [-r] [-c]"
    exit 1
    ;;
  esac
done
set +e
# Set some paths and prime system to save environment variables
save () {
  (echo -n "$1=\""; eval "echo -n \"\$$1\""; echo '"') >> environment.sh
}
GC=
RC=
NC=
if tty -s < /dev/fd/1 2> /dev/null; then
  GC='\e[0;32m'
  RC='\e[0;31m'
  NC='\e[0m'
fi
log () {
  d=$(date)
  echo -e "${GC}[$d]${NC} $@"
}
fail () {
  d=$(date)
  echo -e "${RC}[$d]${NC} $@"
  exit 1
}
signaled () {
  fail Interrupted
}
trap signaled TERM QUIT INT
rm -f environment.sh; touch environment.sh

# To run tasks in parallel
#run_bg () {
#  semaphore -j $NUM_THREADS --id masurca_$$ -- "$@"
#}
#run_wait () {
#  semaphore -j $NUM_THREADS --id masurca_$$ --wait
#}
export PATH="/home/shared/programs/MaSuRCA-3.3.0/bin/../CA8/Linux-amd64/bin:/home/shared/programs/MaSuRCA-3.3.0/bin:$PATH"
save PATH
export PERL5LIB=/home/shared/programs/MaSuRCA-3.3.0/bin/../lib/perl${PERL5LIB:+:$PERL5LIB}
save PERL5LIB
NUM_THREADS=16
save NUM_THREADS
log 'Processing pe library reads'
rm -rf meanAndStdevByPrefix.pe.txt
echo 'pe 300 30' >> meanAndStdevByPrefix.pe.txt
rename_filter_fastq 'pe' <(exec expand_fastq '/home/mathu/matepair_mapping/filtered_PE_geo_R1.fastq' | awk '{if(length($0>250)) print substr($0,1,250); else print $0;}') <(exec expand_fastq '/home/mathu/matepair_mapping/filtered_PE_geo_R2.fastq' | awk '{if(length($0>250)) print substr($0,1,250); else print $0;}' ) > 'pe.renamed.fastq'
log 'Processing sj library reads'
rm -rf meanAndStdevByPrefix.sj.txt
echo 'sh 500 100' >> meanAndStdevByPrefix.sj.txt
rename_filter_fastq 'sh' <(exec expand_fastq '/home/mathu/matepair_mapping/out_MP1_reads1.fastq' | awk '{if(length($0>250)) print substr($0,1,250); else print $0;}') <(exec expand_fastq '/home/mathu/matepair_mapping/out_MP2_reads2.fastq' | awk '{if(length($0>250)) print substr($0,1,250); else print $0;}' ) > 'sh.renamed.fastq'

head -q -n 40000  pe.renamed.fastq | grep --text -v '^+' | grep --text -v '^@' > pe_data.tmp
export PE_AVG_READ_LENGTH=`awk '{if(length($1)>31){n+=length($1);m++;}}END{print int(n/m)}' pe_data.tmp`
save PE_AVG_READ_LENGTH
log Average PE read length $PE_AVG_READ_LENGTH
KMER=`for f in pe.renamed.fastq;do head -n 80000 $f |tail -n 40000;done | perl -e 'while($line=<STDIN>){$line=<STDIN>;chomp($line);push(@lines,$line);for($i=0;$i<6;$i++){$line=<STDIN>;}}$min_len=100000;$base_count=0;foreach $l(@lines){$base_count+=length($l);push(@lengths,length($l));@f=split("",$l);foreach $base(@f){if(uc($base) eq "G" || uc($base) eq "C"){$gc_count++}}} @lengths =sort {$b <=> $a} @lengths; $min_len=$lengths[int($#lengths*.75)];  $gc_ratio=$gc_count/$base_count;$kmer=0;if($gc_ratio>=0.35 && $gc_ratio<=0.6){$kmer=int($min_len*.66);}else{$kmer=int($min_len*.33);} $kmer++ if($kmer%2==0); $kmer=31 if($kmer<31); $kmer=127 if($kmer>127); print $kmer'`
save KMER
log Using kmer size of $KMER for the graph
KMER_J=31
save KMER_J
MIN_Q_CHAR=`cat pe.renamed.fastq |head -n 50000 | awk 'BEGIN{flag=0}{if($0 ~ /^\+/){flag=1}else if(flag==1){print $0;flag=0}}'  | perl -ne 'BEGIN{$q0_char="@";}{chomp;@f=split "";foreach $v(@f){if(ord($v)<ord($q0_char)){$q0_char=$v;}}}END{$ans=ord($q0_char);if($ans<64){print "33\n"}else{print "64\n"}}'`
save MIN_Q_CHAR
log MIN_Q_CHAR: $MIN_Q_CHAR
JF_SIZE=`ls -l *.fastq | awk '{n+=$5}END{s=int(n/50); if(s>20000000000)printf "%.0f",s;else print "20000000000";}'`
save JF_SIZE
perl -e '{if(int('$JF_SIZE')>20000000000){print "WARNING: JF_SIZE set too low, increasing JF_SIZE to at least '$JF_SIZE', this automatic increase may be not enough!\n"}}'
log Creating mer database for Quorum
awk '{print substr($0,1,200)}' pe.renamed.fastq | quorum_create_database -t 16 -s $JF_SIZE -b 7 -m 24 -q $((MIN_Q_CHAR + 5)) -o quorum_mer_db.jf.tmp /dev/stdin && mv quorum_mer_db.jf.tmp quorum_mer_db.jf
if [ 0 != 0 ]; then
  fail Increase JF_SIZE in config file, the recommendation is to set this to genome_size*coverage/2
fi

log Error correct PE

quorum_error_correct_reads  -q $((MIN_Q_CHAR + 40)) --contaminant=/home/shared/programs/MaSuRCA-3.3.0/bin/../share/adapter.jf -m 1 -s 1 -g 1 -a 3 -t 16 -w 10 -e 3 -M  quorum_mer_db.jf pe.renamed.fastq --no-discard -o pe.cor.tmp --verbose 1>quorum.err 2>&1 && mv pe.cor.tmp.fa pe.cor.fa || fail Error correction of PE reads failed. Check pe.cor.log.

log Error correct JUMP

quorum_error_correct_reads -q $((MIN_Q_CHAR + 40)) --contaminant=/home/shared/programs/MaSuRCA-3.3.0/bin/../share/adapter.jf -m 1 -s 1 -g 2 -a 3 -t 16 -w 10 -e 3 -M  quorum_mer_db.jf sh.renamed.fastq --no-discard -o sj.cor.tmp --verbose 1>quorum.err 2>&1 && mv sj.cor.tmp.fa sj.cor.fa || fail Error correction of JUMP reads failed. Check sj.cor.log.

if [ -s ESTIMATED_GENOME_SIZE.txt ];then
ESTIMATED_GENOME_SIZE=`head -n 1 ESTIMATED_GENOME_SIZE.txt`
else
log Estimating genome size
jellyfish count -m 31 -t 16 -C -s $JF_SIZE -o k_u_hash_0 pe.cor.fa sj.cor.fa
export ESTIMATED_GENOME_SIZE=`jellyfish histo -t 16 -h 1 k_u_hash_0 | tail -n 1 |awk '{print $2}'`
echo $ESTIMATED_GENOME_SIZE > ESTIMATED_GENOME_SIZE.txt
fi
save ESTIMATED_GENOME_SIZE
log "Estimated genome size: $ESTIMATED_GENOME_SIZE"

log Creating k-unitigs with k=$KMER
create_k_unitigs_large_k -c $(($KMER-1)) -t 16 -m $KMER -n $(($ESTIMATED_GENOME_SIZE*2)) -l $KMER -f `perl -e 'print 1/'$KMER'/1e5'` pe.cor.fa sj.cor.fa  | grep --text -v '^>' | perl -ane '{$seq=$F[0]; $F[0]=~tr/ACTGactg/TGACtgac/;$revseq=reverse($F[0]); $h{($seq ge $revseq)?$seq:$revseq}=1;}END{$n=0;foreach $k(keys %h){print ">",$n++," length:",length($k),"\n$k\n"}}' > guillaumeKUnitigsAtLeast32bases_all.fasta.tmp && mv guillaumeKUnitigsAtLeast32bases_all.fasta.tmp guillaumeKUnitigsAtLeast32bases_all.fasta
if [[ $KMER -eq $KMER_J ]];then
ln -s guillaumeKUnitigsAtLeast32bases_all.fasta guillaumeKUnitigsAtLeast32bases_all.jump.fasta
else
log Creating k-unitigs with k=$KMER_J
create_k_unitigs_large_k -c $(($KMER_J-1)) -t 16 -m $KMER_J -n $(($ESTIMATED_GENOME_SIZE*2)) -l $KMER_J -f `perl -e 'print 1/'$KMER_J'/1e5'` pe.cor.fa sj.cor.fa  | grep --text -v '^>' | perl -ane '{$seq=$F[0]; $F[0]=~tr/ACTGactg/TGACtgac/;$revseq=reverse($F[0]); $h{($seq ge $revseq)?$seq:$revseq}=1;}END{$n=0;foreach $k(keys %h){print ">",$n++," length:",length($k),"\n$k\n"}}' > guillaumeKUnitigsAtLeast32bases_all.jump.fasta.tmp && mv guillaumeKUnitigsAtLeast32bases_all.jump.fasta.tmp guillaumeKUnitigsAtLeast32bases_all.jump.fasta 
fi

log 'Filtering mate pairs'
rm -rf work2
createSuperReadsForDirectory.perl  -maxnodes 2000 -minreadsinsuperread 1 -l $KMER_J -join-aggressive 1 -mean-and-stdev-by-prefix-file meanAndStdevByPrefix.sj.txt -kunitigsfile guillaumeKUnitigsAtLeast32bases_all.jump.fasta -t 16 -mikedebug work2 sj.cor.fa 1> super2.err 2>&1
if [[ ! -e work2/superReads.success ]];then
fail Super reads failed, check super2.err and files in ./work2/
fi
JUMP_READ_LEN=`head -n 100000 sj.cor.fa | grep -v '^@' |awk 'BEGIN{m=0}{if(length($1)>m) m=length($1);}END{print m}'`
if [ ! $KMER_J -eq $KMER ] && [ $JUMP_READ_LEN -gt $KMER ];then
createSuperReadsForDirectory.perl  -maxnodes 2000 -minreadsinsuperread 1 -l $KMER -join-aggressive 1 -mean-and-stdev-by-prefix-file meanAndStdevByPrefix.sj.txt -kunitigsfile guillaumeKUnitigsAtLeast32bases_all.fasta -t 16 -mikedebug work2.1 sj.cor.fa 1> super2.1.err 2>&1
if [[ ! -e work2.1/superReads.success ]];then
fail Super reads failed, check super2.1.err and files in ./work2.1/
fi
fi
filter_alt.pl outtie < work2/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt >  chimeric_sj.txt 
if [ ! $KMER_J -eq $KMER ];then
filter_alt.pl outtie < work2.1/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt >>  chimeric_sj.txt 
fi
filter_redundancy.pl 2 < work2/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt > redundant_sj.txt
echo 'Chimeric/Redundant jump reads:';wc -l  chimeric_sj.txt redundant_sj.txt;
ufasta extract -v -f <(cat chimeric_sj.txt redundant_sj.txt) sj.cor.fa | ufasta extract -f <(awk '{
			prefix=substr($1,1,2); 
			readnumber=int(substr($1,3));  
			if(readnumber%2==0){
				last_readnumber=readnumber; 
				last_prefix=prefix;
			}else{
				if(last_readnumber==readnumber-1 && last_prefix==prefix){
					print prefix""last_readnumber"\n"prefix""readnumber;
				}
			}
			}' work2/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt) /dev/stdin | awk '{print $1}' > sj.cor.clean.fa
rm -f sj.cor.clean.rev.fa
grep --text -A 1 '^>sh' sj.cor.clean.fa | grep --text -v '^\-\-'  | reverse_complement  >> sj.cor.clean.rev.fa
ln -sf sj.cor.clean.rev.fa sj.cor.ext.fa
log 'Creating FRG files'
rm -rf compute_jump_coverage.txt
echo -n "5000 " >> compute_jump_coverage.txt
grep --text -A 1 '^>sh' sj.cor.ext.fa | grep --text -v '^\-\-' > sh.tmp
error_corrected2frg sh 5000 200 2000000000 sh.tmp | grep --text '^{LKG' |wc -l >> compute_jump_coverage.txt
JUMP_BASES_COVERED=`awk 'BEGIN{b=0}{b+=$1*$2;}END{print b}' compute_jump_coverage.txt`
save JUMP_BASES_COVERED
grep --text -A 1 '^>sh' sj.cor.ext.fa | grep --text -v '^\-\-' | sample_mate_pairs.pl 300 `perl -e 'print int('$JUMP_BASES_COVERED'/'$ESTIMATED_GENOME_SIZE'/1.6)'` 1 > sh.tmp
error_corrected2frg sh 5000 200 2000000000 sh.tmp > sh.cor.clean.frg
rm -f sh.tmp

log 'Computing super reads from PE '
rm -rf work1
CA_DIR="CA";
createSuperReadsForDirectory.perl -l $KMER -mean-and-stdev-by-prefix-file meanAndStdevByPrefix.pe.txt -kunitigsfile guillaumeKUnitigsAtLeast32bases_all.fasta -t 16 -mikedebug work1 pe.cor.fa 1> super1.err 2>&1
if [[ ! -e work1/superReads.success ]];then
fail Super reads failed, check super1.err and files in ./work1/
fi
create_sr_frg.pl 65535 < work1/superReadSequences.fasta 2>/dev/null | fasta2frg.pl super >  superReadSequences_shr.frg.tmp && mv superReadSequences_shr.frg.tmp superReadSequences_shr.frg

echo 1 > PLOIDY.txt

log 'Celera Assembler'
rm -rf CA
echo "gkpFixInsertSizes=0
merylThreads=16
merylMemory=32768
ovlStoreMemory=32768
ovlThreads=2
frgCorrThreads=2
frgCorrConcurrency=16 
ovlCorrConcurrency=16 
ovlConcurrency=16
useGrid=0
gridEngine=SGE
unitigger=bogart
utgGraphErrorLimit=1000
utgMergeErrorLimit=1000
utgGraphErrorRate=0.015
utgMergeErrorRate=0.025
ovlCorrBatchSize=100000
doUnitigSplitting=0
cgwDemoteRBP=0
doChimeraDetection=normal
merylThreads=16
computeInsertSize=0
cnsOnGrid=0
cnsConcurrency=16
cnsMinFrags=10000
cnsMaxCoverage=7
cnsReuseUnitigs=1
cgwErrorRate=0.1" > runCA.spec
runCA -s runCA.spec stopAfter=initialStoreBuilding -p genome -d CA cgwErrorRate=0.15 doFragmentCorrection=0 doOverlapBasedTrimming=0 doExtendClearRanges=0 ovlMerSize=30 superReadSequences_shr.frg   sh.cor.clean.frg    1> runCA0.out 2>&1
TOTAL_READS=`gatekeeper -dumpinfo -lastfragiid CA/genome.gkpStore | awk '{print $NF}'`
save TOTAL_READS
ovlRefBlockSize=`perl -e '$s=int('$TOTAL_READS'/5); if($s>100000){print $s}else{print "100000"}'`
save ovlRefBlockSize
ovlHashBlockLength=10000000
save ovlHashBlockLength
ovlCorrBatchSize=`perl -e '$s=int('$TOTAL_READS'/100); if($s>10000){print $s}else{print "10000"}'`
save ovlCorrBatchSize
echo "ovlRefBlockSize=$ovlRefBlockSize 
ovlHashBlockLength=$ovlHashBlockLength
ovlCorrBatchSize=$ovlCorrBatchSize
" >> runCA.spec

rm -f CA/0-overlaptrim-overlap/overlap.sh CA/1-overlapper/overlap.sh CA/3-overlapcorrection/frgcorr.sh CA/3-overlapcorrection/ovlcorr.sh CA/5-consensus/consensus.sh

runCA -s runCA.spec stopAfter=consensusAfterUnitigger -p genome -d CA cgwErrorRate=0.15 doFragmentCorrection=0 doOverlapBasedTrimming=0 doExtendClearRanges=0 ovlMerSize=30 superReadSequences_shr.frg   sh.cor.clean.frg    1> runCA1.out 2>&1


if [[ -e "CA/4-unitigger/unitigger.err" ]];then
  log "Overlap/unitig success"
else
  fail Overlap/unitig failed, check output under CA/ and runCA1.out
fi

if [ ! -e CA/recompute_astat.success ];then
  log "Recomputing A-stat for super-reads"
  recompute_astat_superreads_CA8.sh genome CA $PE_AVG_READ_LENGTH work1/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt superReadSequences_shr.frg
fi

if [ ! -e CA/overlapFilter.success ];then
NUM_SUPER_READS=`cat superReadSequences_shr.frg  | grep -c --text '^{FRG' `
save NUM_SUPER_READS
log "Filtering overlaps"
( cd CA && 
tigStore -g genome.gkpStore -t genome.tigStore 5 -U -d consensus | 
awk -F "=" 'BEGIN{print ">unique unitigs";flag=0}{if($1 ~ /^>/){if($6>=5){flag=1}}else{if(flag){print $1"N"}flag=0}}' | 
jellyfish count -L 2 -C -m 30 -s $ESTIMATED_GENOME_SIZE -t 16 -o unitig_mers /dev/fd/0 && 
cat <(overlapStore -b 1 -e $NUM_SUPER_READS -d genome.ovlStore  | awk '{if($1<$2 && ($1<'$NUM_SUPER_READS' && $2<'$NUM_SUPER_READS')) print $0}'|filter_overlap_file -t 16 <(gatekeeper  -dumpfragments -withsequence genome.gkpStore| grep -P '^fragmentIdent|^fragmentSequence' | awk 'BEGIN{flag=1}{if(flag){print ">"$3}else{ print $3;} flag=1-flag; }') unitig_mers /dev/fd/0) <(overlapStore -d genome.ovlStore | awk '{if($1<$2 && ($1>='$NUM_SUPER_READS' || $2>='$NUM_SUPER_READS')) print $1" "$2" "$3" "$4" "$5" "$6" "$7}')  |convertOverlap -ovl |gzip > overlaps_dedup.ovb.gz &&  
overlapStoreBuild -o genome.ovlStore.BUILDING -M 32768 -g genome.gkpStore overlaps_dedup.ovb.gz 1>overlapStore.rebuild.err 2>&1 &&  
rm -rf ovlStoreBackup && 
mkdir ovlStoreBackup && 
mv 4-unitigger 5-consensus 5-consensus-coverage-stat 5-consensus-insert-sizes genome.tigStore genome.ovlStore ovlStoreBackup && 
mv genome.ovlStore.BUILDING genome.ovlStore && rm recompute_astat.success && touch overlapFilter.success
)
runCA -s runCA.spec stopAfter=consensusAfterUnitigger -p genome -d CA cgwErrorRate=0.15 doFragmentCorrection=0 doOverlapBasedTrimming=0 doExtendClearRanges=0 ovlMerSize=30 superReadSequences_shr.frg   sh.cor.clean.frg    1> runCA2.out 2>&1 
log "Recomputing A-stat for super-reads"
recompute_astat_superreads_CA8.sh genome CA $PE_AVG_READ_LENGTH work1/readPlacementsInSuperReads.final.read.superRead.offset.ori.txt superReadSequences_shr.frg
fi

runCA -s runCA.spec cgwErrorRate=0.15 -p genome -d CA doFragmentCorrection=0 doOverlapBasedTrimming=0 doExtendClearRanges=0 ovlMerSize=30 1>runCA3.out 2>&1
if [[ -e "CA/9-terminator/genome.qc" ]];then
  log "CA success"
else
  fail CA failed, check output under CA/ and runCA3.out
fi
if [ ! -d $CA_DIR ];then
  fail "mega-reads exited before assembly"
fi
TERMINATOR="9-terminator"
if [ -s $CA_DIR/9-terminator/genome.scf.fasta ];then
  NSCF=`grep --text '^>'  $CA_DIR/9-terminator/genome.scf.fasta |wc -l`
  NCTG=`grep --text '^>'  $CA_DIR/9-terminator/genome.ctg.fasta |wc -l`
  if [ $NCTG -eq $NSCF ];then
    log 'No gap closing possible'
  else
    TERMINATOR="10-gapclose"
    if [ -s $CA_DIR/10-gapclose/genome.scf.fasta ];then
      log 'Gap closing done'
    else
      log 'Gap closing'
      closeGapsLocally.perl --max-reads-in-memory 1000000000 -s 20000000000 --Celera-terminator-directory $CA_DIR/9-terminator --reads-file 'pe.renamed.fastq' --reads-file 'sh.renamed.fastq' --output-directory $CA_DIR/10-gapclose --min-kmer-len 17 --max-kmer-len $(($PE_AVG_READ_LENGTH-5)) --num-threads 16 --contig-length-for-joining $(($PE_AVG_READ_LENGTH-1)) --contig-length-for-fishing 200 --reduce-read-set-kmer-size 21 1>gapClose.err 2>&1
      if [[ -e "$CA_DIR/10-gapclose/genome.ctg.fasta" ]];then
        log 'Gap close success'
      else
        fail Gap close failed, you can still use pre-gap close files under $CA_DIR/9-terminator/. Check gapClose.err for problems.
      fi
    fi
  fi
else
  fail "Assembly stopped or failed, see $CA_DIR.log"
fi
if [ -s $CA_DIR/$TERMINATOR/genome.scf.fasta ];then
  if [ ! -e $CA_DIR/filter_map.contigs.success ];then
  log 'Removing redundant scaffolds'
    PLOIDY=`cat PLOIDY.txt`
    deduplicate_contigs.sh $CA_DIR genome 16 $PLOIDY $TERMINATOR && log "Assembly complete, final scaffold sequences are in $CA_DIR/final.genome.scf.fasta"
  else
  log "Assembly complete, final scaffold sequences are in $CA_DIR/final.genome.scf.fasta"
  fi
else
  fail "Assembly stopped or failed, see $CA_DIR.log"
fi
log 'All done'
log "Final stats for $CA_DIR/final.genome.scf.fasta"
ufasta n50 -A -S -C -E -N50 $CA_DIR/final.genome.scf.fasta
